log_format access_log_format '[$time_local] $request_id $status $request_time $request_length:$bytes_sent $scheme://$host$request_uri  cache=$sent_http_blot_cache ip=$remote_addr st=$upstream_response_time lrs=$limit_req_status ua=$http_user_agent';

# Define whitelist
# I verified the whitelist works by adding limit=$limit to the log format
# then checking that requests from bunny were 'limit=' i.e. empty while
# other requests the limit variable was not, e.g. limit=\x22\xAE...
geo $whitelist {
    default 0;
    # mac server IP
    192.168.1.228 1;
    {{#cdn_ips}}
    {{ip}} 1;
    {{/cdn_ips}}
}

map $whitelist $limit {
    0     $binary_remote_addr;
    1     "";
}

limit_conn_zone $limit zone=connlimit:10m;

limit_req_zone $limit zone=bots:10m rate=1r/s;

# This is for human traffic, but should catch out bots who are scraping too aggressively
limit_req_zone $limit zone=general:10m rate=3r/s;

# This limits requests to a specific hostname, regardless of IP and prevents one site
# consuming too much of the wider server's resources

# Exclude cdn and site from host-specific rate limiting
map $host $host_limit_key {
    default $host;
    cdn.{{host}} "";
    {{host}} "";
}

limit_req_zone $host_limit_key zone=hostlimit:10m rate=5r/s;

error_log {{{log_directory}}}/error.log info;
access_log {{{log_directory}}}/access.log access_log_format;

# Hide the nginx version in the server header
server_tokens off;

# Added to set the content-type charset header 
# for text files served by NGINX
charset utf-8;

{{> static-file.conf}}

{{#lua_package_path}}

# Required to resolve the luarocks module 'resty.auto-ssl'
# when running openresty on Github Actions for some reason
lua_package_path '{{{lua_package_path}}};;';

{{/lua_package_path}}

{{^lua_package_path}}
lua_package_path '{{{config_directory}}}/?.lua;;';
{{/lua_package_path}}

{{> init.conf}}

# Make sure this directory exists
# It does not need to be owned by root
# https://serverfault.com/questions/1029358/nginx-keys-zone-size-persistence-and-maximum-number-of-files
# 1m = ~8000 items
# I believe the nginx memory cache does not need to be as large as cacher_dictionary because it can fall back 
# to disk in an elegant way
proxy_cache_path  {{{cache_directory}}}  levels=1:2 keys_zone=PROXY_CACHE:30m inactive=1y max_size=200g use_temp_path=off;

lua_shared_dict cacher_dictionary 75m;

# used by /clients to handle syncs (this can be computationally expensive)
upstream blot_node {
  # Use least connections algorithm to better distribute load
  least_conn;
  # Activates the cache for connections to upstream servers.
  keepalive 120;

  # We use the default fail_timeout=10s and max_fails=1 since
  # there is a 'backup' server which should never be marked as
  # unavailable using the max_fails=0 directive. We don't actually
  # use NGINX's backup label here since there's only one regular
  # upstream â€“ and proxy_next_upstream only seems to use regular 
  # upstreams, avoiding backups?
  server 127.0.0.1:8089 weight=100;           # blot-container-green MASTER server
  server 127.0.0.1:8088 weight=1 max_fails=0; # blot-container-blue FAILOVER server
}

upstream blot_blogs_node {
  # Use least connections algorithm to better distribute load
  least_conn;
  # Activates the cache for connections to upstream servers.
  keepalive 120;

  # We use the default fail_timeout=10s and max_fails=1 since
  # there is a backup server which is never marked as unavailable
  # We bias traffic to one container slightly so if there is a memory
  # leak both containers do not go down close together
  server 127.0.0.1:8090 weight=100;           # blot-container-yellow
  server 127.0.0.1:8088 backup max_fails=0; # blot-container-blue FAILOVER server
}

upstream blot_dashboard_node {
  # Use least connections algorithm to better distribute load
  least_conn;
  # Activates the cache for connections to upstream servers.
  keepalive 120;

  # We use the default fail_timeout=10s and max_fails=1 since
  # there is a backup server which is never marked as unavailable
  # We bias traffic to one container slightly so if there is a memory
  # leak both containers do not go down close together
  server 127.0.0.1:8088 weight=100;           # blot-container-blue
  server 127.0.0.1:8089 backup max_fails=0;   # blot-container-green FAILOVER server
}

upstream blot_stats {
  server 127.0.0.1:19999;
}